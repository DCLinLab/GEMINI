{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tifffile\n",
    "from aicsimageio import AICSImage\n",
    "from aicsimageio.readers import CziReader\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.feature import canny\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.transform import rotate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import hough_line, hough_line_peaks, rotate as skimage_rotate\n",
    "from skimage.measure import label, regionprops\n",
    "import tifffile\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.ndimage import shift as nd_shift\n",
    "import pandas as pd\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.ndimage import gaussian_filter1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_and_rotate_image(image, angle, center_of_mass):\n",
    "  \n",
    "    image_center = np.array([image.shape[0] / 2, image.shape[1] / 2])\n",
    "    translation = image_center - np.array(center_of_mass)\n",
    "\n",
    "    \n",
    "    translated_image = nd_shift(image, shift=translation, mode='constant', cval=0)\n",
    "\n",
    "  \n",
    "    rotated_image = rotate(translated_image, angle, resize=True, mode='constant', cval=0)\n",
    "\n",
    "    return rotated_image\n",
    "\n",
    "def rotate_all_channels(image_stack, angle, center_of_mass):\n",
    "    rotated_stack = np.array([translate_and_rotate_image(image, angle, center_of_mass) for image in image_stack])\n",
    "    return rotated_stack\n",
    "\n",
    "def transpose_all_channels(image_stack):\n",
    "    transposed_stack = np.array([np.transpose(image) for image in image_stack])\n",
    "    return transposed_stack\n",
    "\n",
    "\n",
    "def get_scale_from_metadata(metadata):\n",
    "\n",
    "    root = ET.fromstring(metadata)\n",
    "\n",
    "\n",
    "    scaling = root.find('.//Scaling/Items')\n",
    "\n",
    " \n",
    "    if scaling is not None:\n",
    "        scaling_x = float(scaling.find('.//Distance[@Id=\"X\"]/Value').text) * 1e6\n",
    "        scaling_y = float(scaling.find('.//Distance[@Id=\"Y\"]/Value').text) * 1e6\n",
    "    else:\n",
    "        scaling_x = None\n",
    "        scaling_y = None\n",
    "\n",
    "    return scaling_x, scaling_y\n",
    "\n",
    "def high_pass_filter(image, intensity_fraction):\n",
    "\n",
    "    threshold_value = intensity_fraction * image.max()\n",
    "\n",
    "    filtered_image = np.where(image > threshold_value, image, 0)\n",
    "\n",
    "    return filtered_image\n",
    "\n",
    "def extract_intensity_profile(image, center):\n",
    "    y_center, x_center = center\n",
    "    profile_length = image.shape[2] \n",
    "    mean_intensities = []\n",
    "\n",
    "    for x in range(x_center, profile_length):\n",
    "        \n",
    "        slice_ = image[:, int(y_center-15):int(y_center+15), x]\n",
    "        mean_intensity = slice_.mean(axis=1) \n",
    "        mean_intensities.append(mean_intensity)\n",
    "\n",
    "    \n",
    "    mean_intensities = np.array(mean_intensities)\n",
    "    \n",
    "    \n",
    "    normalized_intensities = (mean_intensities - mean_intensities.min(axis=0)) / \\\n",
    "                             (mean_intensities.max(axis=0) - mean_intensities.min(axis=0))\n",
    "    \n",
    "    \n",
    "    distances = np.arange(x_center, profile_length) * 1\n",
    "\n",
    "    \n",
    "    cutoff_index = np.where((normalized_intensities < 0.05).all(axis=1))[0]\n",
    "    if cutoff_index.size > 0:\n",
    "        distances = distances[:cutoff_index[0]]\n",
    "        mean_intensities = mean_intensities[:cutoff_index[0]]\n",
    "\n",
    "    return distances, mean_intensities\n",
    "\n",
    "def process_image_for_profile(tiff_path, channel_num):\n",
    "    with tifffile.TiffFile(tiff_path) as tif:\n",
    "       \n",
    "        if len(tif.pages) > 1:\n",
    "            \n",
    "            image_data = np.stack([page.asarray() for page in tif.pages])\n",
    "        else:\n",
    "            \n",
    "            image_data = tif.asarray()\n",
    "    \n",
    "   \n",
    "    center_of_mass = (image_data.shape[1] // 2, image_data.shape[2] // 2)\n",
    "    \n",
    "    \n",
    "    distances, profiles = extract_intensity_profile(image_data[:channel_num], center_of_mass)\n",
    "    distances = distances - distances[0]\n",
    "\n",
    "   \n",
    "    df = pd.DataFrame(profiles, index=distances, columns=[f'Channel {i}' for i in range(channel_num)])\n",
    "    \n",
    "\n",
    "    csv_path = tiff_path.replace('.tiff', '_profile.csv')\n",
    "    df.to_csv(csv_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_czi_files(folder_path, center_ref_channel, edge_ref_channel, cannysigma=1.0):\n",
    "    \n",
    "    output_folder = f\"{folder_path}_Rot\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    czi_files = [f for f in os.listdir(folder_path) if f.endswith('.czi')]\n",
    "\n",
    "    for file_name in czi_files:\n",
    "        \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "       \n",
    "        image = AICSImage(file_path)\n",
    "        metadata = image.metadata\n",
    "        \n",
    "        image_data = image.get_image_data().squeeze()\n",
    "      \n",
    "        filtered_images = np.array([median_filter(image_data[i], size=3) for i in range(image_data.shape[0])])\n",
    "\n",
    "       \n",
    "        thresh_center = threshold_otsu(filtered_images[center_ref_channel])\n",
    "        binary_center = filtered_images[center_ref_channel] > thresh_center\n",
    "        labeled_center = label(binary_center)\n",
    "        regions = regionprops(labeled_center)\n",
    "       \n",
    "        largest_region = max(regions, key=lambda r: r.area)\n",
    "        center_of_mass = largest_region.centroid\n",
    "\n",
    "       \n",
    "        high_passed_image = high_pass_filter(filtered_images[edge_ref_channel], 0.25)\n",
    "        thresh_edge = threshold_otsu(high_passed_image)\n",
    "        binary_edge = filtered_images[edge_ref_channel] > thresh_edge\n",
    "        edges = canny(binary_edge, sigma=cannysigma)\n",
    "        tested_angles = np.linspace(-np.pi / 2, np.pi / 2, 360)\n",
    "        h, theta, d = hough_line(edges, theta=tested_angles)\n",
    "        _, angle, dist = next(zip(*hough_line_peaks(h, theta, d)))\n",
    "\n",
    "        \n",
    "      \n",
    "        rotation_angle_degrees = np.rad2deg(angle) % 360\n",
    "        rotated_images = rotate_all_channels(filtered_images, rotation_angle_degrees, center_of_mass)\n",
    "\n",
    "        output_file_path = os.path.join(output_folder, file_name.replace('.czi', '.tiff'))\n",
    "\n",
    "     \n",
    "        with tifffile.TiffWriter(output_file_path, bigtiff=True) as tiff_writer:\n",
    "            for i in range(rotated_images.shape[0]):\n",
    "                \n",
    "                tiff_writer.save(rotated_images[i])\n",
    "\n",
    "        \n",
    "        num_channels = rotated_images.shape[0]\n",
    "        fig, axes = plt.subplots(1, num_channels, figsize=(num_channels * 5, 5))\n",
    "        for i in range(num_channels):\n",
    "            ax = axes[i] if num_channels > 1 else axes\n",
    "            ax.imshow(rotated_images[i], cmap='gray')\n",
    "            ax.set_title(f'Channel {i}')\n",
    "            ax.axis('off')\n",
    "        plt.suptitle(f'Rotated Channels for {file_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = 'xxx'  \n",
    "\n",
    "center_ref_channel = 2 \n",
    "edge_ref_channel = 1 \n",
    "process_czi_files(folder_path, center_ref_channel, edge_ref_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "def plot_intensity_profiles(df, tiff_filename):\n",
    "    # Plot each channel\n",
    "    for column in df.columns:\n",
    "        plt.plot(df.index, df[column], label=column)\n",
    "\n",
    "    plt.xlabel('Distance (pixels)')\n",
    "    plt.ylabel('Normalized Intensity')\n",
    "    plt.title(f'Intensity Profile for {tiff_filename}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "folder_path = 'xxx_Rot'  \n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.tiff'):\n",
    "        tiff_path = os.path.join(folder_path, file_name)\n",
    "        profile_df = process_image_for_profile(tiff_path, 3)\n",
    "        if profile_df is not None:\n",
    "            plot_intensity_profiles(profile_df, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "   \n",
    "    center_region = data.iloc[0:10]\n",
    "    avg_intensities = center_region[['Channel 0', 'Channel 1', 'Channel 2']].mean()\n",
    "\n",
    "\n",
    "    for channel in ['Channel 0', 'Channel 1', 'Channel 2']:\n",
    "        data[channel] = data[channel] - avg_intensities[channel]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batch_process(folder_path, processed_folder):\n",
    "    if not os.path.exists(processed_folder):\n",
    "        os.makedirs(processed_folder)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            data = process_file(file_path)\n",
    "            processed_filename = filename.split('.csv')[0] + '_processed.csv'\n",
    "            data.to_csv(os.path.join(processed_folder, processed_filename), index=False)\n",
    "\n",
    "folder_path = 'xxx_Rot'\n",
    "processed_folder = 'xxx_csv'\n",
    "batch_process(folder_path, processed_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smooth_data(data, sigma=10):\n",
    "    \"\"\" Apply Gaussian smoothing to a data series. \"\"\"\n",
    "    return gaussian_filter1d(data, sigma)\n",
    "\n",
    "def ThresholdMethod(data, channel, target_intensity):\n",
    "    \"\"\" Find the first point where the channel intensity reaches the target intensity. \"\"\"\n",
    "    data[channel] = (data[channel] - data[channel].min()) / (data[channel].max() - data[channel].min())\n",
    "    return np.where(data[channel] >= target_intensity)[0][0]\n",
    "\n",
    "def TangentMethod(data, channel, percentage):\n",
    "    channel_data = data[channel]\n",
    "    half_max_value = percentage * channel_data.max()\n",
    "    \n",
    "    half_max_index = np.argmax(channel_data >= half_max_value)\n",
    "    \n",
    "   \n",
    "    window_size = 3 \n",
    "    start_index = max(0, half_max_index - window_size)\n",
    "    end_index = min(len(channel_data), half_max_index + window_size + 1) \n",
    "    \n",
    "    x = np.arange(start_index, end_index)\n",
    "    y = channel_data[start_index:end_index]\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    \n",
    "    intersection_point = -intercept / slope\n",
    "    \n",
    "    return int(intersection_point)\n",
    "\n",
    "\n",
    "def calculate_info(df, time):\n",
    "    df['Inferred_Time'] = (df['second_point']**3 - df['first_point']**3) / (df['third_point']**3 - df['first_point']**3) * 11\n",
    "    df['Drug_Time'] = time\n",
    "    return df\n",
    "\n",
    "def analyze_profiles(folder_path, a, b, c):\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            try:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                for channel in data.columns[1:]:  \n",
    "                    data[channel] = smooth_data(data[channel])\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                data = pd.read_csv(file_path)\n",
    "\n",
    "                first_point = TangentMethod(data, 'Channel 3', a)\n",
    "                second_point = TangentMethod(data, 'Channel 1', b)\n",
    "                third_point = TangentMethod(data, 'Channel 2', c)\n",
    "                forth_point = ThresholdMethod(data, 'Channel 2', 0.99)\n",
    "\n",
    "                results.append({\n",
    "                    'filename': filename,\n",
    "                    'first_point': first_point,\n",
    "                    'second_point': second_point,\n",
    "                    'third_point': third_point,\n",
    "                    'forth_point': forth_point\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "                continue  \n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "t = analyze_profiles('folder_path', 0.5, 0.5, 0.5)\n",
    "Drug_Time = 1\n",
    "t_translate = calculate_info(t, Drug_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ptitprince as pt\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import kruskal\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "df = pd.read_csv('/Users/eugeneyan/Desktop/genimi_Data&Code/Fig2/DyeSwitch(Fig2ij)/Source2ij/DyeSwitch_RainCloud.csv')\n",
    "\n",
    "\n",
    "cloud_color = \"grey\" \n",
    "rain_color = \"grey\"   \n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "pt.RainCloud(x = 'Drug_Time', y='Inferred_Time', data=df, \n",
    "             palette=[rain_color],  box_showfliers=False, bw=0.2, width_viol = .7, ax = ax, orient = 'h' , alpha = .35, dodge = True)\n",
    "\n",
    "\n",
    "for artist in ax.artists:\n",
    "    artist.set_alpha(0.5)\n",
    "\n",
    "for line in ax.lines:\n",
    "    line.set_alpha(0.5)\n",
    "\n",
    "\n",
    "for box in ax.patches:\n",
    "    box.set_alpha(0.5)\n",
    "    \n",
    "\n",
    "ax.tick_params(axis='x', direction='in')\n",
    "ax.tick_params(axis='y', direction='in')\n",
    "\n",
    "plt.ylabel('Ground Truth (h)')\n",
    "plt.xlabel('Inferred Time(h)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kruskal_result = kruskal(*[group[\"Inferred_Time\"].values for name, group in df.groupby(\"Drug_Time\")])\n",
    "\n",
    "dunn_result = sp.posthoc_dunn(df, val_col=\"Inferred_Time\", group_col=\"Drug_Time\", p_adjust=\"bonferroni\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))  \n",
    "sns.heatmap(dunn_result, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title(\"Heatmap of Pairwise p-values from Dunn's Test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'Inferred_Time'  \n",
    "dataframe = df\n",
    "\n",
    "data = [df[column_name].dropna() for df in dataframe]\n",
    "std_devs = [df.std() for df in data]\n",
    "std_devs_scaled = [df.std() * 60 for df in data]\n",
    "\n",
    "labels = ['2', '3', '4', '5', '6', '7', '8']\n",
    "\n",
    "filled_marker_style = dict(marker='o', linestyle=':', markersize=15,\n",
    "                           color='darkgrey',\n",
    "                           markerfacecolor='tab:blue',\n",
    "                           markerfacecoloralt='lightsteelblue',\n",
    "                           markeredgecolor='brown')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "plt.plot(labels, std_devs_scaled, color='black', marker='o', ms=5, mfc='w') \n",
    "\n",
    "\n",
    "\n",
    "ax.tick_params(axis='x', direction='in')\n",
    "ax.tick_params(axis='y', direction='in')\n",
    "plt.title('Standard Deviation of Inferred Times')\n",
    "plt.xlabel('Ground Truth (h)', fontsize = 20)\n",
    "plt.ylabel('Standard Deviation (min)')\n",
    "plt.ylim(30, 120)\n",
    "plt.grid(False)  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "\n",
    "\n",
    "data['time_difference'] = data['Inferred_Time'] - data['Drug_Time']\n",
    "data['absolute_diff'] = np.abs(data['time_difference'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "jittered_x = data['Drug_Time'] + np.random.uniform(-0.2, 0.2, size=len(data))\n",
    "\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    x=jittered_x,\n",
    "    y=data['absolute_diff'],\n",
    "    c=data['absolute_diff'],\n",
    "    cmap=\"coolwarm\",\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Time Difference (Color Coded)', fontsize=12)\n",
    "\n",
    "\n",
    "plt.title('Time Difference', fontsize=14)\n",
    "plt.xlabel('Drug Time', fontsize=12)\n",
    "plt.ylabel('Time Difference (Inferred - Ground Truth)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
